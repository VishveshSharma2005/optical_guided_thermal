{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b75ebd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, logging, tarfile, random\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import Resampling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81dc82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSL4EO_DIR   = \"data_ssl4eo\"\n",
    "SCENES_ROOT  = os.path.join(SSL4EO_DIR, \"scenes\")   # will search recursively for all_bands.tif\n",
    "MODELS_DIR   = \"models\"\n",
    "\n",
    "# kept ONLY as future hooks (not used now, but available for pretraining warm-start if you want)\n",
    "ECOSTRESS_DIR    = \"data_ecostress\"\n",
    "PROCESSED_DIR    = \"data_processed\"\n",
    "RAW_DIR          = \"data_raw\"\n",
    "ECO_BEST         = os.path.join(MODELS_DIR, \"ecostress_pretrained_best.pth\")\n",
    "ECO_LAST         = os.path.join(MODELS_DIR, \"ecostress_pretrained_last.pth\")\n",
    "\n",
    "os.makedirs(SSL4EO_DIR,  exist_ok=True)\n",
    "os.makedirs(SCENES_ROOT, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR,  exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a95f7",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "* We are doing 2x super-resolution \n",
    "* For every batch sample: \n",
    "    * HR patch is 128x128\n",
    "    * LR patch is 64x64 (128/2)\n",
    "* `PHYS_LAMBDA` controls how strong the physics-aware loss is compared to pixel MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac896574",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPSCALE        = 2         # 2× or 4× are both allowed by PS; here 2× (you can change to 4)\n",
    "HR_PATCH       = 128       # HR patch size\n",
    "LR_PATCH       = HR_PATCH // UPSCALE\n",
    "BATCH_SIZE     = 4\n",
    "NUM_EPOCHS     = 50\n",
    "LEARNING_RATE  = 1e-4\n",
    "PHYS_LAMBDA    = 0.1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84b661",
   "metadata": {},
   "source": [
    "### What is a scene?\n",
    "* A scene is a full satellite image.\n",
    "* The data set has 17,500 training scenes and each scene is huge \n",
    "* For example if an image is 3000 x 3000 pixels then the scene is huge\n",
    "\n",
    "### What is a Patch? \n",
    "* A patch is a small crop from that big image \n",
    "* For example a patch can be 128 x 128 pixels \n",
    "* Instead of training on the whole scene (Which is impossible due to memory) we cut the scene into multiple patches \n",
    "\n",
    "### Why not cut all possible patches? \n",
    "* Because one scene can yield hundreds or millions of patches \n",
    "* And 17,500 training scenes can create billions of patches \n",
    "\n",
    "### Our solution \n",
    "* Each epoch, we choose only 4 random locations from each scene \n",
    "* So in 1 Epoch:\n",
    "    * `Training Scenes` :  17,500\n",
    "    * `PATCHES_PER_SCENE` =  4\n",
    "    * `TOTAL_PATCHES_PER_EPOCH` = 17,500 x 4 = 70,000\n",
    "* Meaning: We will train on 70,000 patches per each epoch \n",
    "* But next epoch, we take different random patches from scene \n",
    "* Over many epochs, the model sees diverse area from each scene \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17efa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCHES_PER_SCENE_TRAIN = 4\n",
    "PATCHES_PER_SCENE_VAL   = 2\n",
    "PATCHES_PER_SCENE_TEST  = 2\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e65201",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s:%(name)s: %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"infranova_ssl4eo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe190617",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSL4EO_URL = (\n",
    "    \"https://huggingface.co/datasets/torchgeo/ssl4eo_l_benchmark/resolve/main/\"\n",
    "    \"ssl4eo_l_oli_tirs_toa_benchmark.tar.gz?download=true\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75347a01",
   "metadata": {},
   "source": [
    "### Bands we use\n",
    "* B2,B3,B4 - Optical RGB guidance \n",
    "* B10,B11 - Thermal IR channels - we treat them as two possible HR thermal signals \n",
    "* We do not use B1, B5–B9 here because:\n",
    "    * B2–B4 are already strong for edges, textures, landcover boundaries → perfect guidance.\n",
    "    * B10/B11 are the actual temperature-sensitive channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18593965",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAND_IDX = {\n",
    "    \"B2\": 2,    # Blue\n",
    "    \"B3\": 3,    # Green\n",
    "    \"B4\": 4,    # Red\n",
    "    \"B10\": 10,  # Thermal IR 1\n",
    "    \"B11\": 11   # Thermal IR 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec1de6",
   "metadata": {},
   "source": [
    "### Normalization \n",
    "* `norm_np(a)`\n",
    "    * Converts data into `float32`\n",
    "    * Replaces NaN/Inf with reasonable numbers using `np.nan_to_num`\n",
    "    * Min-Max normalizes to [0,1] per patch \n",
    "    * IF patch is nearly constant `(mx - mn < 1e-6)`, returns a zeros array \n",
    "\n",
    "* This ensures:\n",
    "    * stable gradients\n",
    "    * consistent PSNR,SSIM meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2442f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_np(a: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Per-band min-max normalization to [0,1] with NaN/Inf protection.\"\"\"\n",
    "    a = np.array(a, dtype=np.float32)\n",
    "    if np.isnan(a).any() or np.isinf(a).any():\n",
    "        a = np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    mn = float(np.nanmin(a))\n",
    "    mx = float(np.nanmax(a))\n",
    "    if mx - mn < 1e-6:\n",
    "        return np.zeros_like(a, dtype=np.float32)\n",
    "    return ((a - mn) / (mx - mn)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3fb13",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "* `compute_metrics(pred,target)`\n",
    "    * Both inputs are expected in [0,1]\n",
    "    * Computes:\n",
    "        1. MSE\n",
    "        2. PSNR = 10 * log10(1 / MSE)  {Clamped to 100 dB if MSE extremly small}\n",
    "        3. RMSE = sqrt(MSE)\n",
    "        4. SSIm using `skimage.ssim(data_range=1.0)`.\n",
    "\n",
    "Metrics such as PSNR, SSIM, and RMSE are computed in normalized form for consistency. The RMSE we report can be directly converted back to Kelvin by multiplying with the original temperature range. For example, if the normalized RMSE is 0.012 and the temperature range is 50K, then the true thermal error is 0.6 Kelvin. This aligns with real-world expectations and ensures physical meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a16b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: np.ndarray, target: np.ndarray):\n",
    "    \"\"\"PSNR / SSIM / RMSE on [0,1] normalized arrays.\"\"\"\n",
    "    pred   = np.nan_to_num(pred,   nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    target = np.nan_to_num(target, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    mse = float(np.mean((pred - target) ** 2))\n",
    "    if not np.isfinite(mse) or mse < 1e-12:\n",
    "        psnr_val = 100.0\n",
    "        rmse_val = 0.0\n",
    "    else:\n",
    "        psnr_val = 10 * math.log10(1.0 / mse)\n",
    "        rmse_val = math.sqrt(mse)\n",
    "    try:\n",
    "        ssim_val = ssim(target, pred, data_range=1.0)\n",
    "    except Exception:\n",
    "        ssim_val = 0.0\n",
    "    return psnr_val, ssim_val, rmse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcdd3d",
   "metadata": {},
   "source": [
    "### Dataset Download and extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef972d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_ssl4eo_scenes(root=SCENES_ROOT):\n",
    "    \"\"\"\n",
    "    Recursively find all scenes that contain `all_bands.tif`.\n",
    "    Your existing extract matches this layout.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(root, \"**\", \"all_bands.tif\")\n",
    "    scene_files = sorted(glob(pattern, recursive=True))\n",
    "    logger.info(f\"Discovered {len(scene_files)} SSL4EO scenes with all_bands.tif\")\n",
    "    return scene_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15545318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_ssl4eo(archive_dir=SSL4EO_DIR, scenes_root=SCENES_ROOT):\n",
    "    \"\"\"\n",
    "    If no scenes found, optionally download the SSL4EO benchmark archive and extract.\n",
    "    If you've already downloaded/extracted, this will just return quickly.\n",
    "    \"\"\"\n",
    "    scenes = discover_ssl4eo_scenes(scenes_root)\n",
    "    if len(scenes) > 0:\n",
    "        return scenes  # already present\n",
    "\n",
    "    logger.info(\"No SSL4EO scenes found. Attempting to download archive...\")\n",
    "    os.makedirs(archive_dir, exist_ok=True)\n",
    "    archive_path = os.path.join(archive_dir, \"ssl4eo_benchmark.tar.gz\")\n",
    "\n",
    "    if not os.path.exists(archive_path):\n",
    "        import requests\n",
    "        with requests.get(SSL4EO_URL, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get(\"content-length\", 0))\n",
    "            logger.info(f\"Downloading SSL4EO archive ({total/1e9:.2f} GB approx)...\")\n",
    "            with open(archive_path, \"wb\") as f, tqdm(\n",
    "                total=total, unit=\"B\", unit_scale=True, desc=\"ssl4eo_download\"\n",
    "            ) as pbar:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "    else:\n",
    "        logger.info(f\"Found existing archive: {archive_path}\")\n",
    "\n",
    "    logger.info(f\"Extracting {archive_path} -> {scenes_root}\")\n",
    "    os.makedirs(scenes_root, exist_ok=True)\n",
    "    with tarfile.open(archive_path, \"r:gz\") as tf:\n",
    "        tf.extractall(path=scenes_root)\n",
    "    logger.info(\"Extraction finished.\")\n",
    "    return discover_ssl4eo_scenes(scenes_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985646af",
   "metadata": {},
   "source": [
    "### SSL4EOPatchDataset - The data pipeline\n",
    "\n",
    "For each training step, the dataset does this:\n",
    "* \"Pick a scene -> pick a random location -> Cut out a 128,128 high resolution patch of thermal + rgb -> downsample thermal to low resolution -> Give the model low resolution thermal +  high resolution RGB -> Predict High resolution thermal (Ground truth).\"\n",
    "\n",
    "So the dataset is a factory that takes big scenes and turns them into millions of tiny supervised examples:\n",
    "* `scene_files` : list of paths likes `.../ssl4eo/scenes/.../some_scene/all_bands.tif.`\n",
    "* `hr_patch = 128` : The output resolution you want the network to produce (HR Thermal)\n",
    "* `upscale = 2` : We pretend LR sensor sees a coarser 64x64 version of the world, and we want to go to 128x128\n",
    "* `lr_patch = 128 // 2 = 64` : Size of the LR patch\n",
    "* `patches_per_scene`: How many random patches you want to extract per scene per epoch\n",
    "\n",
    "This keeps:\n",
    "1. Training faster\n",
    "2. Per-scene contribution balanced \n",
    "3. And still covers diverse locations across epochs\n",
    "\n",
    "\n",
    "\n",
    "#### `def __len__(self)` - how many patches per epoch:\n",
    "Example: \n",
    "* len(scene_files) = 25,000 -> Total SSL4EO scenes\n",
    "* Train split = 17,500 scenes\n",
    "* PATCHES_PER_SCENE_TRAIN = 4\n",
    "\n",
    "So for the training dataset:\n",
    "* Length = 17500 x 4 = 70,000 patches per epoch \n",
    "* Batch Size = 4 -> steps per epoch = 70,000 / 4 = 17,500 iteratiions\n",
    "\n",
    "So even though the scenes are large, each epoch only uses a subsampled set of patches\n",
    "\n",
    "#### `def _read_bands()` -  reading the needed channels\n",
    "* all_bands.tif has many bands (B1-B11)\n",
    "* We don't need all of them \n",
    "* We select only \n",
    "    * B2,B3,B4 -> RGB \n",
    "    * B10,B11 -> Thermal\n",
    "Output Shape = (5,H,W):\n",
    "* bands[0]: B2 (blue)\n",
    "* bands[1]: B3 (green)\n",
    "* bands[2]: B4 (red)\n",
    "* bands[3]: B10 (thermal 1)\n",
    "* bands[4]: B11 (thermal 2)\n",
    "\n",
    "\n",
    "#### `def __getitem__` - How one patch is created\n",
    "1. Choose which scene this index belongs to\n",
    "* scene_idx = idx // self.patches_per_scene\n",
    "* scene_path = self.scene_files[scene_idx]\n",
    "\n",
    "__getitem__ – how one patch is created\n",
    "\n",
    "This is the important flow. Let’s go line by line.\n",
    "\n",
    "* 4.1 Choose which scene this index belongs to\n",
    "    * scene_idx = idx // self.patches_per_scene\n",
    "    * scene_path = self.scene_files[scene_idx]\n",
    "* 4.2 Read bands\n",
    "    * bands = self._read_bands(scene_path)   # (5, H, W)\n",
    "    * rgb = bands[0:3, :, :]                 # (3, H, W)   B2,B3,B4\n",
    "    * t10 = bands[3, :, :]                   # (H, W)\n",
    "    * t11 = bands[4, :, :]                   # (H, W)\n",
    "SO now we have: \n",
    "1. rgb = 3 channel high res optical\n",
    "2. t10,t11 = two thermal channels \n",
    "\n",
    "* 4.3 Pick which thermal band to supervise with:\n",
    "    *   `if random.random() < 0.5:`\n",
    "    *        `thermal_hr = t10`\n",
    "    *   `else:`\n",
    "    *        `thermal_hr = t11`\n",
    "\n",
    "For this patch:\n",
    "* Sometimes we use B10 as ground truth\n",
    "* Sometimes we use B11\n",
    "\n",
    "Why?\n",
    "1. Both bands are thermal, buth with slight spectral differences\n",
    "2. Randomizing teaches the model to be robust and not overfit to just one band\n",
    "3. Over many epoch, it 'sees' both\n",
    "\n",
    "\n",
    "* 4.4 Normalize optical and thermal\n",
    "    * rgb_n = np.stack([norm_np(rgb[c]) for c in range(3)], axis=0) # (3, H, W)\n",
    "    * thr_n = norm_np(thermal_hr)                                   # (H, W)\n",
    "Each band is mapped to [0,1] using norm_np:\n",
    "1. removes NaN/Infs\n",
    "2. min-> 0, max -> 1\n",
    "This stablizes training and makes metrics well defined \n",
    "\n",
    "* 4.5 Ensure scene big enough - padding if needed\n",
    "    * H, W = thr_n.shape\n",
    "    * if H < self.hr_patch or W < self.hr_patch:\n",
    "        * pad_y = max(0, self.hr_patch - H)\n",
    "        * pad_x = max(0, self.hr_patch - W)\n",
    "        * thr_n = np.pad(thr_n, ((0, pad_y), (0, pad_x)), mode='reflect')\n",
    "        * rgb_n = np.pad(rgb_n, ((0, 0), (0, pad_y), (0, pad_x)), mode='reflect')\n",
    "        * H, W = thr_n.shape\n",
    "If the scene is smaller than 128×128:\n",
    "* We pad using reflection (mirror the edges).\n",
    "* This avoids black borders and keeps patterns natural.\n",
    "\n",
    "\n",
    "#### `Synthesizing the LR thermal (fake low-res sensor)`\n",
    "Now we simulate what a coarser thermal sensor would see. \n",
    "* H_lr, W_lr = H // self.upscale, W // self.upscale\n",
    "* lr_full = F.interpolate(\n",
    "    * torch.from_numpy(thr_n).unsqueeze(0).unsqueeze(0).float(),  # (1,1,H,W)\n",
    "    * size=(H_lr, W_lr),\n",
    "    * mode=\"bilinear\",\n",
    "    * align_corners=False\n",
    "* ).squeeze().numpy()  # (H_lr, W_lr)\n",
    "\n",
    "1. Input: HR thermal thr_n with shape (H, W) → add batch + channel → (1, 1, H, W).\n",
    "2. Downsample to (H/2, W/2) if UPSCALE=2.\n",
    "3. Output: lr_full is the synthetic low-res thermal.\n",
    "\n",
    "So: \n",
    "* thr_n = what a hypothetical super - high resolution thermal sensor would measure\n",
    "* lr_full = what the real lower - res satellite thermal band would see\n",
    "We teach the model to go from `lr_full` back up to `thr_n`, guided by high-res RGB\n",
    "\n",
    "#### `Random crop – aligned HR + LR patches`\n",
    "Now we select a random HR patch and align its LR counterparts\n",
    "* max_y = H - self.hr_patch\n",
    "* max_x = W - self.hr_patch\n",
    "* if max_y <= 0 or max_x <= 0:\n",
    "    * y = 0\n",
    "    * x = 0\n",
    "* else:\n",
    "    * y = np.random.randint(0, max_y + 1)\n",
    "    * x = np.random.randint(0, max_x + 1)\n",
    "Pick a valid top-left corner (y, x) so that [y:y+128, x:x+128] fits.\n",
    "\n",
    "* 6.1 Crop HR thermal and RGB\n",
    "    * hr_t_patch   = thr_n[y:y + self.hr_patch, x:x + self.hr_patch]   # (128,128)\n",
    "    * hr_rgb_patch = rgb_n[:, y:y + self.hr_patch, x:x + self.hr_patch]# (3,128,128)\n",
    "So: \n",
    "1. hr_t_patch: high-res target thermal patch.\n",
    "2. hr_rgb_patch: aligned high-res optical patch.\n",
    "\n",
    "* 6.2 Crop alogned LR thermal patch\n",
    "    * ly, lx = y // self.upscale, x // self.upscale\n",
    "    * lr_t_patch = lr_full[ly:ly + self.lr_patch, lx:lx + self.lr_patch]  # (64,64)\n",
    "if upscale = 2\n",
    "1. 1 LR picel = 2 HR pixel\n",
    "2. So we divide y and x by 2\n",
    "Now we have a fully aligned triplet\n",
    "1. LR thermal = `lr_t_patch` (64x64)\n",
    "2. HR RGB = `hr_rgb_patch` (3,128,128)\n",
    "3. HR thermal = `hr_t_patch` (128x128)\n",
    "This is exactly the relationship the model is supposed to learn \n",
    "\n",
    "#### `Safety shape checks`\n",
    "* There's some shape cleaning.\n",
    "* This just protects against weird border cases (like tiny off by one due to divide/round)\n",
    "\n",
    "#### `Convert to tensors for the model`\n",
    "* lr_t  = torch.from_numpy(lr_t_patch).unsqueeze(0).float()      # (1, LR, LR)\n",
    "* hr_rgb = torch.from_numpy(hr_rgb_patch).float()                # (3, HR, HR)\n",
    "* hr_t   = torch.from_numpy(hr_t_patch).unsqueeze(0).float()     # (1, HR, HR)\n",
    "* return lr_t, hr_rgb, hr_t\n",
    "\n",
    "Final shapes\n",
    "* lr_t: (1, 64, 64) → low-res thermal input.\n",
    "* hr_rgb: (3, 128, 128) → high-res optical guidance.\n",
    "* hr_t: (1, 128, 128) → ground truth high-res thermal.\n",
    "When we wrap this in a DataLoader with batch_size=BATCH_SIZE, PyTorch adds the batch dimension:\n",
    "* lr_t: (B, 1, 64, 64)\n",
    "* hr_rgb: (B, 3, 128, 128)\n",
    "* hr_t: (B, 1, 128, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef84063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSL4EOPatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For each scene:\n",
    "      - Loads all_bands.tif\n",
    "      - Uses B2/B3/B4 as HR optical guidance (3×channels)\n",
    "      - Uses either B10 or B11 (chosen randomly) as HR thermal \"truth\"\n",
    "      - Synthesizes LR thermal by downsampling with factor UPSCALE\n",
    "      - Returns aligned patches: lr_thermal (1, LR, LR), hr_rgb (3, HR, HR), hr_thermal (1, HR, HR)\n",
    "    The length is (#scenes * patches_per_scene) so each epoch samples a fixed number\n",
    "    of random patches per scene, instead of exploding to millions of iterations.\n",
    "    \"\"\"\n",
    "    def __init__(self, scene_files, hr_patch=HR_PATCH, upscale=UPSCALE,\n",
    "                 patches_per_scene=4, mode=\"train\"):\n",
    "        super().__init__()\n",
    "        self.scene_files = list(scene_files)\n",
    "        self.hr_patch = hr_patch\n",
    "        self.lr_patch = hr_patch // upscale\n",
    "        self.upscale = upscale\n",
    "        self.patches_per_scene = patches_per_scene\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scene_files) * self.patches_per_scene\n",
    "\n",
    "    def _read_bands(self, scene_path):\n",
    "        # rasterio uses 1-based band indices\n",
    "        with rasterio.open(scene_path) as src:\n",
    "            # order = [B2,B3,B4,B10,B11]\n",
    "            bands = src.read([\n",
    "                BAND_IDX[\"B2\"], BAND_IDX[\"B3\"], BAND_IDX[\"B4\"],\n",
    "                BAND_IDX[\"B10\"], BAND_IDX[\"B11\"]\n",
    "            ]).astype(np.float32)  # shape (5, H, W)\n",
    "        return bands\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # map global index -> which scene\n",
    "        scene_idx = idx // self.patches_per_scene\n",
    "        scene_path = self.scene_files[scene_idx]\n",
    "\n",
    "        bands = self._read_bands(scene_path)   # (5, H, W)\n",
    "        rgb = bands[0:3, :, :]                 # B2,B3,B4\n",
    "        t10 = bands[3, :, :]                   # B10\n",
    "        t11 = bands[4, :, :]                   # B11\n",
    "\n",
    "        # Pick one thermal band per patch (so model learns both across epochs)\n",
    "        if random.random() < 0.5:\n",
    "            thermal_hr = t10\n",
    "        else:\n",
    "            thermal_hr = t11\n",
    "\n",
    "        # Normalize optical & thermal\n",
    "        rgb_n = np.stack([norm_np(rgb[c]) for c in range(3)], axis=0)       # (3,H,W)\n",
    "        thr_n = norm_np(thermal_hr)                                        # (H,W)\n",
    "\n",
    "        H, W = thr_n.shape\n",
    "        # if scene smaller than patch, reflect-pad\n",
    "        if H < self.hr_patch or W < self.hr_patch:\n",
    "            pad_y = max(0, self.hr_patch - H)\n",
    "            pad_x = max(0, self.hr_patch - W)\n",
    "            thr_n = np.pad(thr_n, ((0, pad_y), (0, pad_x)), mode='reflect')\n",
    "            rgb_n = np.pad(rgb_n, ((0, 0), (0, pad_y), (0, pad_x)), mode='reflect')\n",
    "            H, W = thr_n.shape\n",
    "\n",
    "        # synthesize LR by downsampling HR thermal\n",
    "        H_lr, W_lr = H // self.upscale, W // self.upscale\n",
    "        lr_full = F.interpolate(\n",
    "            torch.from_numpy(thr_n).unsqueeze(0).unsqueeze(0).float(),  # (1,1,H,W)\n",
    "            size=(H_lr, W_lr),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze().numpy()  # (H_lr, W_lr)\n",
    "\n",
    "        # random crop on HR, with aligned LR crop\n",
    "        max_y = H - self.hr_patch\n",
    "        max_x = W - self.hr_patch\n",
    "        if max_y <= 0 or max_x <= 0:\n",
    "            y = 0\n",
    "            x = 0\n",
    "        else:\n",
    "            y = np.random.randint(0, max_y + 1)\n",
    "            x = np.random.randint(0, max_x + 1)\n",
    "\n",
    "        hr_t_patch   = thr_n[:, y:y + self.hr_patch,] if thr_n.ndim == 3 else thr_n[y:y + self.hr_patch,\n",
    "                                                                                    x:x + self.hr_patch]\n",
    "        hr_rgb_patch = rgb_n[:, y:y + self.hr_patch, x:x + self.hr_patch]\n",
    "\n",
    "        ly, lx = y // self.upscale, x // self.upscale\n",
    "        lr_t_patch = lr_full[ly:ly + self.lr_patch, lx:lx + self.lr_patch]\n",
    "\n",
    "        # ensure correct shapes\n",
    "        hr_t_patch = hr_t_patch if hr_t_patch.ndim == 2 else hr_t_patch[0]\n",
    "        if hr_t_patch.shape != (self.hr_patch, self.hr_patch):\n",
    "            # safety crop\n",
    "            hr_t_patch = hr_t_patch[:self.hr_patch, :self.hr_patch]\n",
    "            hr_rgb_patch = hr_rgb_patch[:, :self.hr_patch, :self.hr_patch]\n",
    "        if lr_t_patch.shape != (self.lr_patch, self.lr_patch):\n",
    "            lr_t_patch = lr_t_patch[:self.lr_patch, :self.lr_patch]\n",
    "\n",
    "        # to tensors\n",
    "        lr_t  = torch.from_numpy(lr_t_patch).unsqueeze(0).float()      # (1, LR, LR)\n",
    "        hr_rgb = torch.from_numpy(hr_rgb_patch).float()                # (3, HR, HR)\n",
    "        hr_t   = torch.from_numpy(hr_t_patch).unsqueeze(0).float()     # (1, HR, HR)\n",
    "\n",
    "        return lr_t, hr_rgb, hr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c25f87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.avgpool(x)\n",
    "        y = self.fc(y)\n",
    "        return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace41d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, max(8, in_channels//2), kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(max(8, in_channels//2), 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        att = self.conv(x)\n",
    "        return x * att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ecea511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCAB(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, reduction=16):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size, padding=pad),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size, padding=pad)\n",
    "        )\n",
    "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
    "        self.res_scale = 0.1\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res = self.ca(res)\n",
    "        return x + res * self.res_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33256609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGroup(nn.Module):\n",
    "    def __init__(self, channels, n_rcab=4):\n",
    "        super().__init__()\n",
    "        layers = [RCAB(channels) for _ in range(n_rcab)]\n",
    "        self.body = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.body(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bfe5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedUpsampler(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale=UPSCALE):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.proj = nn.Conv2d(in_channels, out_channels * (scale*scale), kernel_size=3, padding=1)\n",
    "        self.post = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x, target_size=None):\n",
    "        x = self.proj(x)\n",
    "        x = F.pixel_shuffle(x, self.scale)\n",
    "        x = self.post(x)\n",
    "        if target_size is not None:\n",
    "            x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cfb71e",
   "metadata": {},
   "source": [
    "### DualEDSRPlus - The Model Architecture\n",
    "\n",
    "Inputs:\n",
    "* xT: Low resolution thermal map, shape (B,1,H_LR,W_LR)\n",
    "* xO: high resoluton optical RGB image, shape (B,3,H_HR,W_HR) where H_HR = upscale x H_LR\n",
    "\n",
    "Output: \n",
    "* out: High-resolution thermal map, shape (B,1,H_HR,W_HR)\n",
    "\n",
    "Conceptually:\n",
    "1. Extract features from thermal and optical seperately\n",
    "2. Upsample thermal features to the optical features\n",
    "3. Fuse Thermal + Optical features with channel and spatial attention\n",
    "4. Refine fused featured and predict a high-res thermal map\n",
    "\n",
    "It is called Dual EDSRPlus because:\n",
    "* Dual: Represents the dual stream of optical and thermal\n",
    "* EDSR: Heavy residual blocks like the EDSR super resolution architecture\n",
    "* PLUS: Extra fusion and attention modules\n",
    "\n",
    "#### 1.The constructor __init__\n",
    "* `n_resgroups`: How many ResidualGroup blocks per stream\n",
    "* `n_rcab`: how many RCAB blocks insideeach group\n",
    "* `n_feats`: Feature channels in the hidden layer\n",
    "* `upscale`: 2 for 2x SR, 4 for 4x etc\n",
    "These hyperparameters control the deapth and width of the network\n",
    "\n",
    "##### 1.1 Input Convolutions\n",
    "* convT_in: First layer for thermal\n",
    "    * Input: 1 Channel (Thermal)\n",
    "    * Output: 64 channels (by default)\n",
    "    * Kernel: 3x3, padding 1 -> preserves spatial size\n",
    "* convO_int: first layer of optical\n",
    "    * Input: 3 Channels (RGB)\n",
    "    * Output: 64 Channels\n",
    "    * Kernel: Same 3x3\n",
    "Intuition: Convert raw images into a shared feature space of 64 channels , but with seperate weights for thermal and optical\n",
    "\n",
    "##### 1.2 Deep feature extraction: Residual groups\n",
    "* Thermal stream: t_groups\n",
    "* Optical stream: o_groups\n",
    "Each `ResidualGroup` contains multiple RCAB's and a group skip connection \n",
    "* RCAB = Conv -> ReLU -> Conv + ChannelAttention + Residual\n",
    "* Several RCAB's in series = deeper representation\n",
    "* Group-level skip = stabilizes training, preserves low level info\n",
    "Effect:\n",
    "* The thermal stream learns rich temperature - related patterns\n",
    "* The optical stream learns edges,textrues,land covers boundaries etc\n",
    "\n",
    "\n",
    "##### 1.3 Learnable upsamples for thermal\n",
    "* Takes low resolution thermal features (B,64,H_LR,W_LR) and upsamples them to (B,64,H_HR,W_HR) using `LearnedUpsampler`\n",
    "Intuition:\n",
    "* Instead of just doing bilinear upsampling on the image, we upsample in feature space with learnable parameters,more powerful and sharp\n",
    "\n",
    "\n",
    "##### 1.4 Fusion + attention modules\n",
    "After upsampling thermal features to match optical resolution, we will:\n",
    "1. Concatenate: `[thermal_up,optical_features]` Along with channels _> 64+64 = 128 channels\n",
    "2. ConvFuse: 1x1 conv to mix and comparess 128 -> 64 channels\n",
    "3. fuse_ca: ChannelAttention -> Decide which channels are important globally\n",
    "4. fuse_sa: SpatialAttention -> Decide which pixels/locations are important\n",
    "\n",
    "ChannelAttenion:\n",
    "* Computes a channel descriptor via global avergae pooling\n",
    "* Passes it through small MLP (conv 1x1) and sigmoid to get weights in [0,1]\n",
    "* Mutiplies each channel by its importance\n",
    "\n",
    "SpatialAttention:\n",
    "* Convs over the feature map to produce a 1-channel importance mask [H,w] in [0,1]\n",
    "* Multplies the whole feature map by this mask -> Focuses on hotspots,boundaries\n",
    "\n",
    "##### 1.5 Refinement + output\n",
    "* `refine`: two extra 3x3 convs with ReLU to clean up fused representation\n",
    "* `convOut`: final 3x3 conv to map features (64 channels) -> 1 channel\n",
    "\n",
    "So final output has shape (B,1,H_HR,W_HR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51057708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEDSRPlus(nn.Module):\n",
    "    def __init__(self, n_resgroups=4, n_rcab=4, n_feats=64, upscale=UPSCALE):\n",
    "        super().__init__()\n",
    "        self.upscale = upscale\n",
    "        self.n_feats = n_feats\n",
    "\n",
    "        self.convT_in = nn.Conv2d(1, n_feats, 3, padding=1)\n",
    "        self.convO_in = nn.Conv2d(3, n_feats, 3, padding=1)\n",
    "\n",
    "        self.t_groups = nn.Sequential(*[ResidualGroup(n_feats, n_rcab) for _ in range(n_resgroups)])\n",
    "        self.o_groups = nn.Sequential(*[ResidualGroup(n_feats, n_rcab) for _ in range(n_resgroups)])\n",
    "\n",
    "        self.t_upsampler = LearnedUpsampler(n_feats, n_feats, scale=upscale)\n",
    "\n",
    "        self.convFuse = nn.Conv2d(2 * n_feats, n_feats, kernel_size=1)\n",
    "        self.fuse_ca  = ChannelAttention(n_feats)\n",
    "        self.fuse_sa  = SpatialAttention(n_feats)\n",
    "\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(n_feats, n_feats, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n_feats, n_feats, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.convOut = nn.Conv2d(n_feats, 1, kernel_size=3, padding=1)\n",
    "\n",
    "        # Kaiming init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, xT, xO):\n",
    "        fT = F.relu(self.convT_in(xT))\n",
    "        fO = F.relu(self.convO_in(xO))\n",
    "\n",
    "        fT = self.t_groups(fT)\n",
    "        fO = self.o_groups(fO)\n",
    "\n",
    "        fT_up_raw = self.t_upsampler(fT)\n",
    "        target_hw = (fO.shape[2], fO.shape[3])\n",
    "        fT_up = F.interpolate(fT_up_raw, size=target_hw, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        f = torch.cat([fT_up, fO], dim=1)\n",
    "        f = F.relu(self.convFuse(f))\n",
    "        f = self.fuse_ca(f)\n",
    "        f = self.fuse_sa(f)\n",
    "        f = self.refine(f)\n",
    "        out = self.convOut(f)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8024b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_load_weights(model, ckpt_path, verbose=False):\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        logger.warning(f\"No checkpoint at {ckpt_path}\")\n",
    "        return 0\n",
    "    src = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    if isinstance(src, dict) and \"model_state\" in src:\n",
    "        src = src[\"model_state\"]\n",
    "    model_dict = model.state_dict()\n",
    "    loaded = 0\n",
    "    for k, v in src.items():\n",
    "        if k in model_dict and model_dict[k].shape == v.shape:\n",
    "            model_dict[k] = v\n",
    "            loaded += 1\n",
    "        elif verbose:\n",
    "            logger.info(f\"Skipping {k}; mismatch or missing.\")\n",
    "    model.load_state_dict(model_dict)\n",
    "    logger.info(f\"Partial-loaded {loaded} tensors from {ckpt_path}\")\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831087e5",
   "metadata": {},
   "source": [
    "### Physics-aware loss - respecting temperature consistency\n",
    "\n",
    "* In training:\n",
    "    * pred_hr = model(lr_t,hr_rgb)\n",
    "    * loss_fid = mse_loss(pred_hr, hr_t)\n",
    "This is the standard super resolution loss: predicted HR vs groud truth HR\n",
    "\n",
    "* Now the physics-aware part: \n",
    "    * pred_lr = F.interpolate(\n",
    "    * pred_hr, size=(lr_t.shape[2], lr_t.shape[3]),\n",
    "    * mode=\"area\"  # average pooling style\n",
    "    * )\n",
    "    * loss_phys = mse_loss(pred_lr, lr_t)\n",
    "    * loss = loss_fid + PHYS_LAMBDA * loss_phys\n",
    "\n",
    "* What is happening?\n",
    "    1. WE take the predicted HR thermal map, and downsample it back to LR using mode = 'area', which is like block averaging\n",
    "    2. That gives an LR map representing average energy/temperature over coarse pixel\n",
    "    3. We force it to be close to the original LR thermal input\n",
    "* Why is this physics-aware?\n",
    "    * Real sensors (e.g TIRS) measure average radiance/temperature over each coarse pixel footprint\n",
    "    * If we \"invent\" crazy fine scae structure that changes the average, we would be violating physics\n",
    "    * This loss says: ` Whatever details you invent at high resolution must still average back to the same coarse thermal value he satellite saw`\n",
    "\n",
    "Our optical guidance can suggest edges and shapes, but the coarse thermal energy constraint keeps temperature in check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4807f62",
   "metadata": {},
   "source": [
    "### Training,Validation, test flow\n",
    "\n",
    "* Scene - level split (no leakage of tiles from same scene across sets)\n",
    "* 70% train, 15% test, 15% validation\n",
    "\n",
    "Each epoch: \n",
    "* For each train scene: 4 random patches\n",
    "* For each val scene: 2 patches\n",
    "* For each test scene: 2 patches\n",
    "So epochs are manageable but still diverse\n",
    "\n",
    "1. Training loop:\n",
    "* Per epoch:\n",
    "    * Iterate `train_loader`.\n",
    "    * For each batch:\n",
    "        1. forawrd -> pred_hr\n",
    "        2. compute loss_fid and loss_phys.\n",
    "        3. Back propogration, gradient clipping, optimizer step\n",
    "2. Validation phase:\n",
    "    * No gradients\n",
    "    * For each val batch:\n",
    "        1. forward -> `out`\n",
    "        2. convert to Numpy, compute PSNR/SSIM/RMSE\n",
    "    * Average metrics across validation patches\n",
    "    * If PSNR improved -> save best model\n",
    "3. Save last model every epoch\n",
    "\n",
    "\n",
    "#### Training evaluation\n",
    "After training: \n",
    "* Reload best model\n",
    "* Run through test_loader exactly like val\n",
    "* Log final PSNR/SSIM/RMSE -> this is what you can compare against literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c5fd802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssl4eo(num_epochs=NUM_EPOCHS):\n",
    "    # 1) Find / download scenes\n",
    "    scenes = discover_ssl4eo_scenes()\n",
    "    if len(scenes) == 0:\n",
    "        scenes = maybe_download_ssl4eo()\n",
    "    if len(scenes) == 0:\n",
    "        logger.error(\"No SSL4EO scenes available; aborting.\")\n",
    "        return\n",
    "\n",
    "    # 2) Train/val/test split\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    scenes_shuffled = scenes.copy()\n",
    "    random.shuffle(scenes_shuffled)\n",
    "\n",
    "    n_total = len(scenes_shuffled)\n",
    "    n_train = int(0.7 * n_total)\n",
    "    n_val   = int(0.15 * n_total)\n",
    "    n_test  = n_total - n_train - n_val\n",
    "\n",
    "    train_scenes = scenes_shuffled[:n_train]\n",
    "    val_scenes   = scenes_shuffled[n_train:n_train + n_val]\n",
    "    test_scenes  = scenes_shuffled[n_train + n_val:]\n",
    "\n",
    "    logger.info(\n",
    "        f\"Starting SSL4EO training with DualEDSRPlus + physics-aware loss.\\n\"\n",
    "        f\"Scene split -> Train: {len(train_scenes)}, Val: {len(val_scenes)}, Test: {len(test_scenes)}\"\n",
    "    )\n",
    "\n",
    "    # 3) Datasets / loaders\n",
    "    train_ds = SSL4EOPatchDataset(\n",
    "        train_scenes, hr_patch=HR_PATCH, upscale=UPSCALE,\n",
    "        patches_per_scene=PATCHES_PER_SCENE_TRAIN, mode=\"train\"\n",
    "    )\n",
    "    val_ds = SSL4EOPatchDataset(\n",
    "        val_scenes, hr_patch=HR_PATCH, upscale=UPSCALE,\n",
    "        patches_per_scene=PATCHES_PER_SCENE_VAL, mode=\"val\"\n",
    "    )\n",
    "    test_ds = SSL4EOPatchDataset(\n",
    "        test_scenes, hr_patch=HR_PATCH, upscale=UPSCALE,\n",
    "        patches_per_scene=PATCHES_PER_SCENE_TEST, mode=\"test\"\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=0, pin_memory=(DEVICE.type == \"cuda\")\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=1, shuffle=False,\n",
    "        num_workers=0, pin_memory=(DEVICE.type == \"cuda\")\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=1, shuffle=False,\n",
    "        num_workers=0, pin_memory=(DEVICE.type == \"cuda\")\n",
    "    )\n",
    "\n",
    "    # 4) Model, optimizer, loss\n",
    "    model = DualEDSRPlus(n_resgroups=4, n_rcab=4, n_feats=64, upscale=UPSCALE).to(DEVICE)\n",
    "\n",
    "    # optional warm-start from ECOSTRESS later:\n",
    "    # partial_load_weights(model, ECO_BEST, verbose=False)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    mse_loss  = nn.MSELoss()\n",
    "\n",
    "    BEST_PATH = os.path.join(MODELS_DIR, \"ssl4eo_best.pth\")\n",
    "    LAST_PATH = os.path.join(MODELS_DIR, \"ssl4eo_last.pth\")\n",
    "\n",
    "    # --------- RESUME LOGIC ---------\n",
    "    best_val_psnr = -1e9\n",
    "    start_epoch = 1\n",
    "\n",
    "    if os.path.exists(LAST_PATH):\n",
    "        try:\n",
    "            ckpt = torch.load(LAST_PATH, map_location=DEVICE)\n",
    "            if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "                model.load_state_dict(ckpt[\"model_state\"])\n",
    "                if \"optimizer_state\" in ckpt:\n",
    "                    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "                if \"epoch\" in ckpt:\n",
    "                    start_epoch = ckpt[\"epoch\"] + 1\n",
    "                best_val_psnr = ckpt.get(\"best_val_psnr\", -1e9)\n",
    "                logger.info(\n",
    "                    f\"Resuming from checkpoint {LAST_PATH}: \"\n",
    "                    f\"start_epoch={start_epoch}, best_val_psnr={best_val_psnr:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                logger.warning(\"Checkpoint format unexpected; starting from scratch.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {LAST_PATH} ({e}); starting from scratch.\")\n",
    "    else:\n",
    "        logger.info(\"No LAST checkpoint found; starting from epoch 1.\")\n",
    "\n",
    "    logger.info(f\"Training epochs: {start_epoch} -> {num_epochs}\")\n",
    "    # --------------------------------\n",
    "\n",
    "    # 5) Training loop\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        it = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} (train)\")\n",
    "        for lr_t, hr_rgb, hr_t in pbar:\n",
    "            lr_t   = lr_t.to(DEVICE)      # (B,1,LR,LR)\n",
    "            hr_rgb = hr_rgb.to(DEVICE)    # (B,3,HR,HR)\n",
    "            hr_t   = hr_t.to(DEVICE)      # (B,1,HR,HR)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_hr = model(lr_t, hr_rgb)\n",
    "\n",
    "            # --- Data fidelity loss (per-pixel MSE at HR) ---\n",
    "            loss_fid = mse_loss(pred_hr, hr_t)\n",
    "\n",
    "            # --- Physics-aware loss: coarse thermal consistency ---\n",
    "            pred_lr = F.interpolate(\n",
    "                pred_hr, size=(lr_t.shape[2], lr_t.shape[3]),\n",
    "                mode=\"area\"\n",
    "            )\n",
    "            loss_phys = mse_loss(pred_lr, lr_t)\n",
    "\n",
    "            loss = loss_fid + PHYS_LAMBDA * loss_phys\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running += float(loss.item())\n",
    "            it += 1\n",
    "            pbar.set_postfix(loss=running / max(1, it))\n",
    "\n",
    "        avg_train_loss = running / max(1, it)\n",
    "        logger.info(f\"Epoch {epoch} TRAIN loss={avg_train_loss:.6f}\")\n",
    "\n",
    "        # 6) Validation\n",
    "        model.eval()\n",
    "        ps_sum = ss_sum = rm_sum = 0.0\n",
    "        cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for lr_t, hr_rgb, hr_t in tqdm(val_loader, desc=f\"Epoch {epoch} (val)\"):\n",
    "                lr_t   = lr_t.to(DEVICE)\n",
    "                hr_rgb = hr_rgb.to(DEVICE)\n",
    "                hr_t   = hr_t.to(DEVICE)\n",
    "\n",
    "                out = model(lr_t, hr_rgb)\n",
    "                pred = out.cpu().squeeze().numpy()\n",
    "                tgt  = hr_t.cpu().squeeze().numpy()\n",
    "                ps, ss, rm = compute_metrics(pred, tgt)\n",
    "                ps_sum += ps; ss_sum += ss; rm_sum += rm; cnt += 1\n",
    "        if cnt > 0:\n",
    "            avg_ps = ps_sum / cnt\n",
    "            avg_ss = ss_sum / cnt\n",
    "            avg_rm = rm_sum / cnt\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch} VAL PSNR={avg_ps:.3f} dB, SSIM={avg_ss:.4f}, RMSE={avg_rm:.6f}\"\n",
    "            )\n",
    "            if avg_ps > best_val_psnr:\n",
    "                best_val_psnr = avg_ps\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model_state\": model.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"best_val_psnr\": best_val_psnr,\n",
    "                    },\n",
    "                    BEST_PATH\n",
    "                )\n",
    "                logger.info(f\"Saved BEST model -> {BEST_PATH} (PSNR={avg_ps:.3f})\")\n",
    "\n",
    "        # always save last (FULL checkpoint for resume)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"best_val_psnr\": best_val_psnr,\n",
    "            },\n",
    "            LAST_PATH\n",
    "        )\n",
    "        logger.info(f\"Saved LAST model -> {LAST_PATH} (epoch={epoch})\")\n",
    "\n",
    "    # 7) Final test evaluation with best model\n",
    "    if os.path.exists(BEST_PATH):\n",
    "        ckpt = torch.load(BEST_PATH, map_location=DEVICE)\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        logger.info(f\"Loaded BEST model from {BEST_PATH} for TEST evaluation.\")\n",
    "\n",
    "    model.eval()\n",
    "    ps_sum = ss_sum = rm_sum = 0.0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for lr_t, hr_rgb, hr_t in tqdm(test_loader, desc=\"TEST\"):\n",
    "            lr_t   = lr_t.to(DEVICE)\n",
    "            hr_rgb = hr_rgb.to(DEVICE)\n",
    "            hr_t   = hr_t.to(DEVICE)\n",
    "\n",
    "            out = model(lr_t, hr_rgb)\n",
    "            pred = out.cpu().squeeze().numpy()\n",
    "            tgt  = hr_t.cpu().squeeze().numpy()\n",
    "            ps, ss, rm = compute_metrics(pred, tgt)\n",
    "            ps_sum += ps; ss_sum += ss; rm_sum += rm; cnt += 1\n",
    "    if cnt > 0:\n",
    "        avg_ps = ps_sum / cnt\n",
    "        avg_ss = ss_sum / cnt\n",
    "        avg_rm = rm_sum / cnt\n",
    "        logger.info(\n",
    "            f\"TEST SUMMARY (SSL4EO, RGB-guided, physics-aware): \"\n",
    "            f\"PSNR={avg_ps:.3f} dB, SSIM={avg_ss:.4f}, RMSE={avg_rm:.6f}\"\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881c5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 07:13:23,007 INFO:infranova_ssl4eo: Discovered 25000 SSL4EO scenes with all_bands.tif\n",
      "2025-12-08 07:13:23,015 INFO:infranova_ssl4eo: Starting SSL4EO training with DualEDSRPlus + physics-aware loss.\n",
      "Scene split -> Train: 17500, Val: 3750, Test: 3750\n",
      "2025-12-08 07:13:26,882 INFO:infranova_ssl4eo: Resuming from checkpoint models\\ssl4eo_last.pth: start_epoch=9, best_val_psnr=39.370\n",
      "2025-12-08 07:13:26,883 INFO:infranova_ssl4eo: Training epochs: 9 -> 50\n",
      "Epoch 9/50 (train): 100%|██████████| 17500/17500 [1:23:51<00:00,  3.48it/s, loss=0.000171]\n",
      "2025-12-08 08:37:18,165 INFO:infranova_ssl4eo: Epoch 9 TRAIN loss=0.000171\n",
      "Epoch 9 (val): 100%|██████████| 7500/7500 [05:27<00:00, 22.93it/s]\n",
      "2025-12-08 08:42:45,201 INFO:infranova_ssl4eo: Epoch 9 VAL PSNR=39.584 dB, SSIM=0.9759, RMSE=0.011122\n",
      "2025-12-08 08:42:45,260 INFO:infranova_ssl4eo: Saved BEST model -> models\\ssl4eo_best.pth (PSNR=39.584)\n",
      "2025-12-08 08:42:45,409 INFO:infranova_ssl4eo: Saved LAST model -> models\\ssl4eo_last.pth (epoch=9)\n",
      "Epoch 10/50 (train):   9%|▉         | 1642/17500 [07:38<1:13:49,  3.58it/s, loss=0.000175]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrain_ssl4eo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mtrain_ssl4eo\u001b[39m\u001b[34m(num_epochs)\u001b[39m\n\u001b[32m    119\u001b[39m loss_phys = mse_loss(pred_lr, lr_t)\n\u001b[32m    121\u001b[39m loss = loss_fid + PHYS_LAMBDA * loss_phys\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    124\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Manas Mehta\\Desktop\\PROJECTS\\gpuenv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Manas Mehta\\Desktop\\PROJECTS\\gpuenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Manas Mehta\\Desktop\\PROJECTS\\gpuenv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_ssl4eo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
